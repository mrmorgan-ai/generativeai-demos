{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec67947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ab2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_file(pdf_path):\n",
    "    \"\"\"\n",
    "    This function opens a PDF file and reads all the text from it.\n",
    "    It's like having a robot that can read every page of a book for you!\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    # Open the PDF file (like opening a book)\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        # Read each page (like turning pages in a book)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            # Extract text from this page\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text + \" \"\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08db503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text, chunk_size=200):\n",
    "    \"\"\"\n",
    "    This breaks our long text into smaller pieces (chunks).\n",
    "    Why? Because it's easier to find specific information in small pieces\n",
    "    than in one giant block of text!\n",
    "    \n",
    "    Think of it like organizing your toys into different boxes\n",
    "    instead of having one huge pile.\n",
    "    \"\"\"\n",
    "    # Split text into words (like separating LEGO pieces)\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    # Group words into chunks of specified size\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        # Take a group of words (like grabbing a handful of LEGO pieces)\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        # Join them back into a sentence\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenization is like taking a sentence and separating each word.\n",
    "    For example: \"The cat sat\" becomes [\"the\", \"cat\", \"sat\"]\n",
    "    \"\"\"\n",
    "    # Convert to lowercase (so \"Cat\" and \"cat\" are treated the same)\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and split into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee9231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb04c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
