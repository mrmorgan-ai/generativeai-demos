{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5aab3d0",
   "metadata": {},
   "source": [
    "# **Text Embeddings**\n",
    "## Create Chunks\n",
    "If you are working with large documents, you can chunk them based on queries you expect.\n",
    "- You can chunk at sentence level or at paragraph level. \n",
    "- You can add context to chunks like the document tittle or include some text before or after the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length, min_length):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if (\n",
    "            len(\" \".join(current_chunk)) < max_length\n",
    "            and len(\" \".join(current_chunk)) > min_length\n",
    "        ):\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    # If the last chunk didn't reach the minimum length, add it anyway\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155810c3",
   "metadata": {},
   "source": [
    "## Search Index\n",
    "When doing retrieval, we will need to build a search index for our knowledge base before we perform search. An index will store our embeddings and can quickly retrieve the most similar chunks even in a large database. We can create our index locally using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "embeddings = flattened_df[\"embeddings\"].to_list()\n",
    "\n",
    "# Create the search index\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm=\"ball_tree\").fit(embeddings)\n",
    "\n",
    "# To query the index, you can use the kneighbors method\n",
    "distances, indices = nbrs.kneighbors(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc9cab",
   "metadata": {},
   "source": [
    "## Re-ranking\n",
    "Once you have queried the database, you might need to sort the results from the most relevant. A reranking LLM utilizes Machine Learning to improve the relevance of search results by ordering them from the most relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13acf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar documents\n",
    "distances, indices = nbrs.kneighbors([query_vector])\n",
    "\n",
    "index = []\n",
    "# Print the most similar documents\n",
    "for i in range(3):\n",
    "    index = indices[0][i]\n",
    "    for index in indices[0]:\n",
    "        print(flattened_df[\"chunks\"].iloc[index])\n",
    "        print(flattened_df[\"path\"].iloc[index])\n",
    "        print(flattened_df[\"distances\"].iloc[index])\n",
    "    else:\n",
    "        print(f\"Index {index} not found in DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"what is a perceptron?\"\n",
    "\n",
    "\n",
    "def chatbot(user_input):\n",
    "    # Convert the question to a query vector\n",
    "    query_vector = create_embeddings(user_input)\n",
    "\n",
    "    # Find the most similar documents\n",
    "    distances, indices = nbrs.kneighbors([query_vector])\n",
    "\n",
    "    # add documents to query  to provide context\n",
    "    history = []\n",
    "    for index in indices[0]:\n",
    "        history.append(flattened_df[\"chunks\"].iloc[index])\n",
    "\n",
    "    # combine the history and the user input\n",
    "    history.append(user_input)\n",
    "\n",
    "    # create a message object\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant that helps with AI questions.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": history[-1]},\n",
    "    ]\n",
    "\n",
    "    # use chat completion to generate a response\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\", temperature=0.7, max_tokens=800, messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message\n",
    "\n",
    "\n",
    "chatbot(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203392b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
