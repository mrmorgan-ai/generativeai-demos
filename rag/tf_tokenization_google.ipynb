{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676a7382",
   "metadata": {},
   "source": [
    "# Tokenizing, indexing and one-hot encoding strings with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ff354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input string data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "data = [\n",
    "\"The earth is spherical.\",\n",
    "\"The earth is a planet.\",\n",
    "\"I like to eat at a restaurant.\"]\n",
    "# Filter the punctiations, tokenize the words and index them to integers\n",
    "tokenizer = Tokenizer(num_words=15, filters='!\"#$%&()*+,-./:;<=>?[\\\\]^_'{|}~\\t\\n', lower=True,\n",
    "split=' ')\n",
    "tokenizer.fit_on_texts(data)\n",
    "# Translate each sentence into its word-level IDs, and then one-hot encode those IDs\n",
    "ID_sequences = tokenizer.texts_to_sequences(data)\n",
    "binary_sequences = tokenizer.sequences_to_matrix(ID_sequences)\n",
    "print(\"ID dictionary:\\n\", tokenizer.word_index)\n",
    "print(\"\\nID sequences:\\n\", ID_sequences)\n",
    "print(\"\\n One-hot encoded sequences:\\n\", binary_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa93c3",
   "metadata": {},
   "source": [
    "# Loading and plotting GloVe and Word2Vec embeddings in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d094f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "def tsne_plot(models, words, seed=23):\n",
    "\"Creates a TSNE models & plots for multiple word models for the given words\"\n",
    "plt.figure(figsize=(len(models)*30, len(models)*30))\n",
    "model_ix = 0\n",
    "for model in models:\n",
    "labels = []\n",
    "tokens = []\n",
    "for word in words:\n",
    "tokens.append(model[word])\n",
    "labels.append(word)\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=seed)\n",
    "new_values = tsne_model.fit_transform(np.array(tokens))\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "x.append(value[0])\n",
    "y.append(value[1])\n",
    "model_ix +=1\n",
    "plt.subplot(10, 10, model_ix)\n",
    "for i in range(len(x)):\n",
    "plt.scatter(x[i],y[i])\n",
    "plt.annotate(labels[i],\n",
    "xy=(x[i], y[i]),\n",
    "xytext=(5, 2),\n",
    "textcoords='offset points',\n",
    "ha='right',\n",
    "va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "v2w_model = api.load('word2vec-google-news-300')\n",
    "glove_model = api.load('glove-twitter-25')\n",
    "print(\"words most similar to 'computer' with word2vec and glove respectively:\")\n",
    "pprint.pprint(v2w_model.most_similar(\"computer\")[:3])\n",
    "pprint.pprint(glove_model.most_similar(\"computer\")[:3])\n",
    "pprint.pprint(\"2d projection of some common words of both models\")\n",
    "sample_common_words= list(set(v2w_model.index_to_key[100:10000])\n",
    "& set(glove_model.index_to_key[100:10000]))[:100]\n",
    "tsne_plot([v2w_model, glove_model], sample_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438fd1b",
   "metadata": {},
   "source": [
    "# Self-supervised Training and inference using Doc2Vec on private corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.Doc2Vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# train model on a sequence of documents tagged with their IDs\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=8, window=3, min_count=1, workers=6)\n",
    "# persist model to disk, and load it to infer on new documents\n",
    "model_file = get_tmpfile(\"Doc2Vec_v1\")\n",
    "model.save(model_file)\n",
    "model = Doc2Vec.load(model_file)\n",
    "model.infer_vector([\"human\", \"interface\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252008bd",
   "metadata": {},
   "source": [
    "# Creating & integrating text embeddings (Vertex, Tfhub) into keras text classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "# Set the model name. For multilingual: use \"text-multilingual-embedding-002\"\n",
    "MODEL_NAME = \"text-embedding-004\"\n",
    "# Set the task_type, text and optional title as the model inputs.\n",
    "# Available task_types are \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\",\n",
    "# \"SEMANTIC_SIMILARITY\", # \"CLASSIFICATION\", and \"CLUSTERING\"\n",
    "TASK_TYPE = \"RETRIEVAL_DOCUMENT\"\n",
    "TITLE = \"Google\"\n",
    "TEXT = \"Embed text.\"\n",
    "# Use Vertex LLM text embeddings\n",
    "embeddings_vx = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\n",
    "def LLM_embed(text):\n",
    "def embed_text(text):\n",
    "text_inp = TextEmbeddingInput(task_type=\"CLASSIFICATION\", text=text.numpy())\n",
    "return np.array(embeddings_vx.get_embeddings([text_inp])[0].values)\n",
    "output = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)\n",
    "output.set_shape((768,))\n",
    "return output\n",
    "# Embed strings using vertex LLMs\n",
    "LLM_embeddings=train_data.map(lambda x,y: ((LLM_embed(x), y))\n",
    "# Embed strings in the tf.dataset using one of the tf hub models\n",
    "embedding = \"https://tfhub.dev/google/sentence-t5/st5-base/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)\n",
    "# Train model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer) # omit this layer if using Vertex LLM embeddings\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(train_data.shuffle(100).batch(8))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
